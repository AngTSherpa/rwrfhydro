---
title: "Precipitation Evaluation"
author: "Arezoo Rafieeinasab & Aubrey Dugger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## background 

To be used for rwrfhydro paper, combined two vignette by Arezoo

# Geospatial Tools 
For most of the postprocessing, there is a need to create spatial maps, aggregate over spatial units and also produce georeference raster and shapefiles. Many of the existing functions in available spatial libraries such as SP, RGDAL, RGEOS and Raster has been wrapped in rwrfhydro to serve our purpose. Here we explain these spatial functions, their application as well as some examples.

## List of the available functions
- GetProj
- GetGeogridSpatialInfo
- ExportGeogrid
- GetGeogridIndex
- GetTimeZone
- GetRfc
- GetPoly
- PolygonToRaster

## General Info
The case study data should be placed at the home directory in order to use the following vignette without changing any line. Otherwise, change the addresses accordingly. To find out the address to the home directory use the following commnad.

```{r}
path.expand("~")
```

Load teh rwrfhydro package.

```{r}
library(rwrfhydro)
```

Set a data path to Fourmile Creek test case.

```{r}
fcPath <- '~/wrfHydroTestCases/Fourmile_Creek_newGMD'
```

Geogrid file is the main file containing all the base geographic information on the model domain such as the geographic coordinate system, latitude, longitude of each pixel and so on. We use this file frequently. Set a path to geogrid file.

```{r}
geoFile <- paste0(fcPath,'/DOMAIN/geo_OrodellBasin_1km_8nlcd11.nc')
```


## GetProj

To be able to use any of the spatial tools in R, projection information of the model domain is required. All the model input and output file are based on geogrid file domain. `GetProj` pull projection information of WRF-Hydro modeling domain from geogrid file. It takes only `geoFile` and return the projection information as a character.

```{r}
proj4 <- GetProj(geoFile)
proj4
```

## GetGeogridSpatialInfo

It pull necessary geospatial information about WRF-Hydro modeling domain from geogrid file used for regridding and deprojection.
It only requires the address to the geogrid file and return a data frame containing geospatial information such as the projection information, number of rows and columns and size of the grids.

```{r}
geoInfo <- GetGeogridSpatialInfo(geoFile)
geoInfo
```


## ExportGeogrid

If you need to create a georeferenced TIF file from any variable in a netcdf file, then you need to use `ExportGeogrid` function. It takes a NetCDF file having lat/lon information and converts the specified variable into a georeferenced TIF file for use in standard GIS tools.
Now, let's export one of the variable from the geogrid file. You can get a list of all available variables in the `geoFile` using `ncdump` function in rwrfhydro.

```{r, eval = FALSE}
head(ncdump(geoFile))
```

Now we will create a georeferenced TIF file from HGT_M field. You only need to provide the address to geogrid file (`geoFile`), the name of the variable (`HGT_M`) and the name of the output file (`geogrid_hgt.tif`).

```{r, results='hide', message=FALSE, warning=FALSE}
ExportGeogrid(geoFile,"HGT_M", "geogrid_hgt.tif")
```

You can now use the created file in any standard GIS platform. Here we will just read it into memory as a raster and dispaly it.

```{r plot1, fig.show = "hold", fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
# read the saved tiff file
r <- raster::raster("geogrid_hgt.tif")

# plot the imported raster from tif file
raster::plot(r, main = "HGT_M")

# check the raser information and notice taht geographic coordinate information has been added.
r
```


Many of the input and output files such as LDASOUT output file does not contain lat/lon coordinates but matches the spatial coordinate system of the geogrid input file. In that case, you could feed the geogrid file `geoFile` which the lat/lon information will be taken from that file. 

```{r plot2, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
file = paste0(fcPath,"/RUN.FLUXCOMP/OUTPUT_ALLRT_MAY13_HOURLY/RESTART.2013060100_DOMAIN1")
# ncdump(file) # check if the SOIL_T exist in the file

# we will read the third layer of soil temperature
ExportGeogrid(file,
             inVar="SOIL_T",
             outFile="20130315_soilm3.tif",
             geoFile=geoFile,
             inLyr=3)

# read the  created tiff file
r <- raster::raster("20130315_soilm3.tif")

# plot the imported raster from tiff file
raster::plot(r, main = "Soil Temperature")

# check the raster information and notice geographic coordinate information has been added
r
```


## GetGeogridIndex

To be able to use a bunch of tools such as `GetMultiNcdf`, one needs to have the indices (x,y) or the location of each cell within the domain. `GetGeogridIndex` get geogrid cell indices from lat/lon (or other) coordinates. `GetGeogridIndex` reads in a set of lat/lon (or other) coordinates and generates a corresponding set of geogrid index pairs. Yo can assign a projection to the points using `proj4` argument which will be used to transform the point to the `geoFile` coordinate system. Ckeck the vignette on precipitation for usage.

```{r}
sg <- data.frame(lon = seq(-105.562, -105.323, length.out = 10), 
                 lat = seq(40.0125, 40.0682, length.out = 10))
GetGeogridIndex(sg, geoFile)
```


## GetTimeZone

Many of the point observation are reported in local time and needs to be converted to UTC time to be comparable with WRF-Hydro input and outputs. `GetTimeZone` return the time zone for any point having longitude and latitude. It simply takes a dataframe containing at least two fields of `latitude` and `longitude`, overlays the `points` with a timezone shapefile (can be downloded from <http://efele.net/maps/tz/world/>). The shapefile is provided in rwrfhydro data and it is called `timeZone`.

```{r}
# timeZone has been provided by rwrfhydro as a SpatialPolygonDataFrame
class(timeZone)

# Shows the available timezone (TZID column in timeZone@data)
head(timeZone@data)
```

Function has three arguments. 

- `points`: A dataframe of the points. The dataframe should contain at least two fields called `latitude` and `longitude`.
- `proj4`: Projection of the `points` to be used in transforming the `points` projection to `timeZone` projection. Default is `+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0` which is the same as the `timezone` projection.
- `parallel`: If the number of points are high you can parallelize the process.

It will return the `points` dataframe with an added column called `timeZone`. It will return NA in case the point is not in any polygon. Now let's generate some random points and find their time zone information.

```{r}
# Provide a dataframe of 10 points having longitude and latitude as column name.
sg <- data.frame(longitude = seq(-110, -80, length.out = 10),
                 latitude = seq(30, 50, length.out = 10))

# Find the time zone for each point
sg <- GetTimeZone(sg)
sg
```

## GetRfc

US has 13 offices of the River Forecast Center (RFC) which issue daily river forecasts using hydrologic models based on rainfall, soil characteristics, precipitation forecasts, and several other variables. Many of the statistics are desired to be grouped into River Forecast Center level so it would be easier to compare with the performance of the RFC models in the past. The RFC boundary shapefile is provided in rwrfhydro data and is called `rfc`.

```{r}
class(rfc)

# Shows the available rfc, name of the column is BASIN_ID
head(rfc@data)
```

`GetRfc` return the RFC name for any point having `longitude` and `latitude`. It takes a dataframe containing at least two fields of `latitude` and `longitude`, overlays the points with a `rfc` SpatialPolygonDataFrame and return the rfc's BASIN_ID. Function has three arguments. 

- `points`: A dataframe of the points. The dataframe should contain at least two fields called "latitude" and "longitude".
- `proj4`: Projection of the `points` to be used in transforming the `points` projection to `rfc` projection. Default is `+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0`.
- `parallel`: If the number of points are high you can parallelize the process.

It will return the points dataframe with an added column called `rfc`. It will return NA in case the point is not in any polygon.

```{r}
# Provide a dataframe of 10 points having longitude and latitude as column name.
sg <- data.frame(longitude = seq(-110, -80, length.out = 10), 
                 latitude = seq(30, 50, length.out = 10))

# Find the rfc for each point
sg <- GetRfc(sg)
sg
```

## GetPoly

`Getpoly` is similar to `GetRfc`, it is a wrapper for function `sp::over`. It takes a dataframe containing at least two fields of `latitude` and `longitude`, overlays the points with a `SpatialPolygonDataFrame` and return the requested attribute from the polygon. One could use the available `SpatialPolygon*` loaded into memory or provide the address to the location of a polygon shapefile and the name of the shapefile and it will read the polygon using `rgdal::readOGR` function.

Let's get the RFC information from `GetPoly` instead of `GetRfc`. Here we provide the name of the `SpatialPolygon*` and using argument `join` request one of the attributes of the polygon. For example, here we have requested the `BASIN_ID`, `RFC_NAME` and `RFC_CITY`. 

```{r}
# Provide a dataframe of 10 points having longitude and latitude
sg <- data.frame(longitude = seq(-110, -80, length.out = 10), 
                 latitude = seq(30, 50, length.out = 10))

# Find the ID of RFC for each point
sg <- GetPoly(points = sg, polygon = rfc, join = "BASIN_ID")

# Find the full name of RFC for each point
sg <- GetPoly(points = sg, polygon = rfc, join = "RFC_NAME")

# Find the location/city of RFC for each point
sg <- GetPoly(points = sg, polygon = rfc, join = "RFC_CITY")
sg
```

Now let's provide the address to a shapefile on the disk as well as the name of the shapefile and perform the same process. We have clipped the `HUC12` shapefile and provided in the case study as a sample. The northeast of the clipped polygon covers partially the Fourmile Creek domain.
 
```{r}
# Provide a dataframe of 10 points within the Fourmile Creek domain having longitude and latitude
sg <- data.frame(longitude = seq(-105.562, -105.323, length.out = 10), 
                 latitude = seq(40.0125, 40.0682, length.out = 10))


# rgdal::readOG` has been used in the GetPoly function and it does not interpret the character/symbol `~`, 
# therefore, we need to use path.expand to get the full address to the case study location on your system. 
polygonAddress <- paste0(path.expand(fcPath), "/polygons")


# Find the HUC12 for each point
sg <- GetPoly(points = sg,
              polygonAddress = polygonAddress,
              polygonShapeFile = "clipped_huc12",
              join = "HUC12")
sg
```

## PolyToRaster

If one wants to create a mask in the model domain (geogrid file), then needs to use `PolyToRaster`. It first picks up the required geographic information (like `proj4`) from the geogrid file (`geoFile`) and then use `raster::rasterize` function to grab the mask or attibute values from the `SpatialPolygonDataFrame`. This function is basically wrapping the `raster::rasterize` fucntion to serve our purpose. Below is a few different way one could use this function.

Example 1 : 
Let's get the RFC's ID for each pixel within the Fourmile Creek domain. This is equivalent to rasterizing the `rfc` `SpatialPolygonDataFrame` based on the `BASIN_ID`.

```{r plot3, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
r <- PolyToRaster(geoFile = geoFile,
                  useRfc = TRUE,
                  field ="BASIN_ID")
```

To know what are the corresponding values to the integer values used in rasterized output, you need to use the following command.

```{r}
r@data@attributes 
```
As the result shows all the case study domain falls into one RFC. 


Example 2 : 
rasterize the HUC12 `SpatialPolygonDataFrame` based on the `HUC12` field. The clipped HUC12 shapefile is provided with the test case which is quite larger than the model domain. You could read the shapefile and plot it as below.
```{r results="hide", plot4, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
polyg <- rgdal::readOGR(paste0(path.expand(fcPath), "/polygons"), "clipped_huc12")
raster::plot(polyg, main = "Clipped HUC12")
```

Our study domain partially covers a few basins at northeast of this shapefile.

```{r results="hide", plot5, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
polygonAddress <- paste0(path.expand(fcPath), "/polygons")
r <- PolyToRaster(geoFile = geoFile,
                  polygonAddress = polygonAddress,
                  polygonShapeFile = "clipped_huc12",
                  field ="HUC12")
```

To get the `HUC12` actual values:

```{r}
r@data@attributes
```

Example 3: You can get a unified mask over the study domain as follows:

```{r results="hide", plot6, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
r <- PolyToRaster(geoFile = geoFile,
                  polygonAddress = polygonAddress,
                  polygonShapeFile = "clipped_huc12",
                  mask =TRUE)
```

Example 4: You could also get a separate mask for each subbasin (HUC12 in this case) with the fraction of each grid cell that is covered by each polygon. The fraction covered is estimated by dividing each cell into 100 subcells and determining presence/absence of the polygon in the center of each subcell. 


```{r results="hide", plot7, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
r <- PolyToRaster(geoFile = geoFile,
                  polygonAddress = polygonAddress,
                  polygonShapeFile = "clipped_huc12",
                  field = "HUC12",
                  getCover = TRUE)
raster::plot(r)
```

# Precipitation Evaluation 

## Background

Forcing are stored in multiple files, either in input forcing files (such as LDASIN or PRECIP_FORCING files) or in the output files (LDASOUT). LDASOUT files contains the variable ACCPRCP which stores the accumulated precipitation, and one can calculate the rainfall depth by subtracting two consecutive time steps. LDASIN and PRECIP_FORCING store rain rate in RAINRATE and precip_rate variables. This vignette serve as a short explanation of how to grab data and do some basic comparisons.

Load the rwrfhydro package. 
```{r  results='hide', message=FALSE, warning=FALSE}
library(rwrfhydro)
```

## Import observed datasets

## GHCN-daily
Global Historical Climatology Network-Daily (GHCN-D) dataset contains daily data from around 80000 surface station in the world, which about two third of them are precipitation only (Menne et al. 2012). It is the most complete collection of U.S. daily data available (Menne et al. 2012). The dataset undergo an automated quality assurance which the details can be found in Durre et al. 2008; 2010. Data is available on http://www1.ncdc.noaa.gov/pub/data/ghcn/daily and is updated frequently. Data is available in two formats either categorized by gauge station or categorized by year. Accordingly, there are two function to pull GHCN-daily data from these two sources called `GetGhcn` and `GetGhcn2`.

### Gauge selection
First step is to select the gauges you want to use for verification based on some criteria. GHCN-daily contains the precipitation data from different sources such as COOP or CoCoRaHS. The selection criteria can be country code, states if country is US, type of rain gauge network (for example CoCoRaHS), or a rectangle domain. 

```{r}
setInternet2(use=FALSE) # If using windows, you may need this.

# Return all the gauges within US from observation network of COOP (C) and CoCoRaHS (1)
countryCodeList <- c("US")
networkCodeList <- c("1","C")
sg <- SelectGhcnGauges(countryCode=countryCodeList,
                       networkCode=networkCodeList)
str(sg)
```

The sg dataframe has all the information provided by NCDC about each gauge. For the rest of this vignette we will use only the domain of Fourmile Creek which is the case study provided. We use the rectangle domain containing Fourmile Creek, as the boundary to collect all the gauges information.

```{r}
sg <- SelectGhcnGauges(domain = TRUE, minLat = 40.0125, maxLat = 40.0682, 
                       minLon = -105.562, maxLon=-105.323)
str(sg)
```

### GetGhcn
GHCN-daily data are archived for each individual gauge in a text file in http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all/. Precipitating can be downloaded for a single site or multiple ones by setting element to "PRCP" and specifying the desired start and end date. Notice, precipitation values are  converted from 10th of mm to mm.

```{r, eval = FALSE}
startDate <- "2013/08/01"
endDate <- "2013/11/01"
element <- "PRCP"
obsPrcp <- GetGhcn(sg$siteIds, element, startDate, endDate, parallel = FALSE)
```

### GetGhcn2
NCDC also provides GHCN-daily categorized by year under http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/. If the number of the gauges are high, `GetGhcn2` is much faster in retrieving data. 

```{r message=FALSE, warning=FALSE}
startDate <- "2013/08/01"
endDate <- "2013/11/01"
element <- "PRCP"
obsPrcp <- GetGhcn2(sg$siteIds, element, startDate, endDate, parallel = FALSE)
head(obsPrcp)
```


### Import forcing/precipitation data used in WRF-Hydro model  
Forcing data used in WRF-Hydro modeling are usually stored in forcing files (such as LDASIN or PRECIP_FORCING files). Here we are going to use the data provided under "Fourmile_Creek" dataset.

Set a data path to the Fourmile Creek test case.

```{r}
fcPath <- '~/wrfHydroTestCases/Fourmile_Creek_newGMD'
```

First make a list of all the forcing files.

```{r}
forcingPath <- paste0(fcPath,"/FORCING")
files <- list.files(path = forcingPath, full.names = TRUE, pattern = glob2rx("2013*LDASIN_DOMAIN1"))
```


In order to be able to pull data from the netcdf files, one needs the location of the points in the geogrid domain file. However, only lat/lon locations of rain gauges are available if using `SelectGhcnGauges` function. Therefore, it is required to map lat/lon information to x/y information in geogrid in order to pull the data from the netcdf files. This can be done using `GetGeogridIndex` function in rwrfhydro. One needs to provide the address to geogrid file, the lat/lon info and the `GetGeogridIndex` function return a dataframe with two column `sn` (south-north) and `ew` (east-west). 

```{r message=FALSE, warning=FALSE}
geoFile <- paste0(fcPath,'/DOMAIN/geo_OrodellBasin_1km_8nlcd11.nc')
rainGgaugeInds <- GetGeogridIndex(xy = data.frame(lon=sg$longitude, lat=sg$latitude),
                                  ncfile = geoFile)
sg <- cbind(sg,rainGgaugeInds)
head(sg)
```

Now we can pull data. One needs to prepare the file, var, and ind variables for `GetMultiNcdf` function (refer to Collect Output Data: GetMultiNcdf vignette . You can leave the stat as mean; since you are pulling data for single pixels,  means return the value of the pixel.

```{r message=FALSE, warning=FALSE}
flList <- list(forcing = files)
varList <- list(forcing = list(PRCP = 'RAINRATE'))
prcpIndex <- list()
for (i in 1:length(sg$siteIds)) {
  if (!is.na(sg$ew[i]) & !is.na(sg$sn[i])) {
    prcpIndex[[as.character(sg$siteIds[i])]] <- list(start=c(sg$ew[i], sg$sn[i],1),
                                                     end=c(sg$ew[i], sg$sn[i],1), stat="mean")
  }
}
indList <-list(forcing = list(PRCP = prcpIndex))
prcpData <- GetMultiNcdf(file = flList, var = varList, ind = indList, parallel=FALSE)
head(prcpData)
```

`GetMultiNcdf` pulls the time information from the netcdf files, if the data is not prepared properly, and the time info is not available, it will return the name of the file instead. In that case, time should be retrieved from the file name which is save in column `POSIXct`. Since the `obsPrcp` data are converted to mm, we also convert the rainrate to rain depth in an hour.

```{r}
prcpData$value<-prcpData$value*3600
```


### Aggregating hourly data into daily.

Each GHCN gauge has a unique reporting time which the daily data is been calculated based on that. The reporting time is archived in the csv files and is retrieved when calling `GetGhcn2` function (you will not get the reporting time using `GetGhcn`). We need to add the reporting time for each point which would be the base for daily aggregation. If there will not be any `reportTime` in `sg` columns, then it uses the default which is 0700 AM. 

```{r}
sg$reportTime <- obsPrcp$reportTime[match(sg$siteIds, obsPrcp$siteIds)]
sg$reportTime[which (sg$reportTime=="" | is.na(sg$reportTime))]<-700
```

Call the `CalcDailyGhcn` function which takes the following steps: 

1. It first search for a column called `timeZone` in the `sg` (selected gauges) dataframe. If the time zone has not been provided, it will call `GetTimeZone(sg)`. To `GetTimeZone` works, `sg` requires to have at least two fields of `latitude` and `longitude`.
1. Having time zone for each gauge, the time offset will be obtained from the `tzLookup` data provided with rwrfhydro. Using the time offset, the UTC time of the precipitation will be converted to Local Standard Time (LST). This is the time convention, GHCN-D data report.
1. The precipitation data will be aggregated based on the reporting time of individual gauge. After the `dailyData` is returned, you can remove the days which do not have full hours reports, `numberOfDataPoints` column has the number of hours that observation was available within a day.

```{r}
names(prcpData)[names(prcpData) == 'value'] <- 'DEL_ACCPRCP'
dailyData <- CalcDailyGhcn(sg = sg,prcp = prcpData)
head(dailyData)
```

### Comparing daily QPE/QPF versus GHCN-D

Final step if to find the common data between the two dataset (precipitation time series (`dailyData`) and the observed GHCN-D (`obsPrcp`)). This can be very fast if using data.table.   

```{r}
#usind data.table merge
common <- data.table:::merge.data.table(data.table::as.data.table(dailyData),data.table::as.data.table(obsPrcp),
                                        by.x=c("ghcnDay","statArg"),
                                        by.y=c("date","siteIds"))
head(common)
```

Call the `CalcStatCont` function and it returns all the requested statistics.  The default are `numPaired` (number of paired data), `meanObs` (mean of observation data), `meanMod` (mean of model/forecast data), `pearsonCor` (Pearson correlation coefficient), `RMSE` (root mean square error), and `multiBias` (multiplicative bias). Here we want to get the statistics for each gauge, therefore, we need to group the data for each gauge. This can be done by defining `groupBy` to be the column name having siteIds, here `statArg`.

```{r plot8, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
stat <- CalcStatCont(DT = common, obsCol = "dailyPrcp", modCol = "value" , groupBy = "statArg")

# CalcStatCont will return a list having two elements of stat and plotList.
names(stat)

#To check the statistics 
stat$stat
```

If the `groupBy` is `NULL` then it will return four informative plots. 

```{r plot9, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
common2 <- common[statArg == unique(statArg)[1]]
stat <- CalcStatCont(DT = common2, obsCol = "dailyPrcp", modCol = "value")
```

You can choose among the four plots by changing the `plot.list` argument. 

```{r plot10, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
stat <- CalcStatCont(DT = common2, obsCol = "dailyPrcp", modCol = "value" , plot.list = "scatterPlot")
```

You can also calculate conditional statistics by defining the boundaries you are interested in. For example, here we calculate the statistics conditioned on the observation to be greater than 1 mm. 

```{r plot11, fig.width = 8, fig.height = 8, out.width='600', out.height='600'}
stat <- CalcStatCont(DT = common2, obsCol = "dailyPrcp", modCol = "value" , 
                     obsCondRange = c(1, Inf), plot.list = "scatterPlot")
```

### Calculate statistics over RFCs

Sometime the verification result at the gauge location is not desired and we want to find the performance of a model over a domain or polygon. If you want to calculate statistics over RFC's, then use `GetRfc` function. One can find out a gauge (point) falls in which RFC using `GetRfc`. You simply feed a dataframe having at least two columns of `latitude` and `longitude` and this functions adds a column to a dataframe with RFC name.

```{r}
# add rfc name
sg <- GetRfc(sg)

# check what is been added
head(sg)
```

Now, add a column to the `common` data having the `rfc` information for each data. And calculate the statistics based on grouping by RFC. 

```{r}
# merge the common data.table with the sg data.frame
common <- data.table:::merge.data.table(common,data.table::as.data.table(sg[, c("siteIds", "rfc")]),
                                        by.x=c("statArg"),
                                        by.y=c("siteIds"))

# calculate statistics using grouping by rfc
stat <- CalcStatCont(DT = common, obsCol = "dailyPrcp", modCol = "value" , 
                     groupBy = "rfc", plot.it = FALSE)

stat$stat
```

As you see above, all the gauges belong to one rfc (`MBRFC`), therefore there will be only one category.

### Calculate statistics over polygons

One can calculate the statistics over any desired polygon shapefile. First, you need to use `GetPoly` function to find each point falls into which polygon. `GetPoly` takes a dataframe containing at least two fields of `latitude` and `longitude`, overlays the points with a `SpatialPolygonDataFrame` and return the requested attribute from the polygon. You can use the available `SpatialPolygon*` loaded into memory or provide the address to the location of a polygon shapefile and the name of the shapefile. 
The clipped HUC12 shapefile is provided with the test case. The northeast of the clipped polygon covers partially the Fourmile Creek domain. Here, we ty to find the corresponding polygon to each gauge and calculate the statistics over those polygons. 

```{r results="hide", message=FALSE, warning=FALSE}
# add HUC12 ids
polygonAddress <- paste0(path.expand(fcPath), "/polygons")
sg <- GetPoly (sg,  polygonAddress = polygonAddress,
               polygonShapeFile = "clipped_huc12",
               join="HUC12")

# check what is been added
head(sg)

# merge the common data.table with the sg data.frame
common <- data.table:::merge.data.table(common,data.table::as.data.table(sg[, c("siteIds","HUC12")]),
                                        by.x=c("statArg"),
                                        by.y=c("siteIds"))

# calculate statistics using grouping by HUC12
stat <- CalcStatCont(DT = common, obsCol = "dailyPrcp", modCol = "value" , 
                     groupBy = "HUC12", plot.it = FALSE)
stat$stat
```

All the gauges with available data belong to one HUC12, therefore there is only one category.

### Calculate categorical statistics

You can also calculate some of the categorical statistics using `CalcStatCategorical` function. It accepts both categorical variable and continuous ones.
If the data is actually categorical, variable `category` should be defined. The elements in `category` will be used as `YES` and `NO` in contingency table. If the data is numeric, then a set of thresholds should be defined. Values exceeding the threshold would be flagged as `YES` and the values below the threshold are considered `NO`. 
You can choose from the available statistics by changing the `statList` argument. By default, it calculates the Hit Rate (H), False Alarm Ratio (FAR) and Critical Success Index (CSI). 
The grouping option is similar to `CalcStatCont`.

```{r}
# calculate categorical statistics
stat <- CalcStatCategorical(DT = common, obsCol = "dailyPrcp", modCol = "value" , 
                            groupBy = "statArg", threshold = c(1,5))
stat
```
