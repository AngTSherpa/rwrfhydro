---
title: "Precipitation Evaluation"
author: "Arezoo Rafieei Nasab"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Background

Forcing are stored in multiple files, either in input forcing files (such as LDASIN or PRECIP_FORCING files) or in the output files (LDASOUT). LDASOUT files contains the variable ACCPRCP which stores the accumulated precipitation, and one can calculate the rainfall depth by subtracting two consecutive time steps. LDASIN and PRECIP_FORCING store rain rate in RAINRATE and precip_rate variables. This vignette serve as an short explanation of how to grab data and do some basin comparisons.

Load the rwrfhydro package. 
```{r  results='hide', message=FALSE, warning=FALSE}
library(rwrfhydro)
```

## Import observed datasets

## GHCN-daily
Global Historical Climatology Network-Daily (GHCN-D) dataset contains daily data from around 80000 surface station in the world, which about two third of them are precipitation only (Menne et al. 2012). It is the most complete collection of U.S. daily data available (Menne et al. 2012). The dataset undergo an automated quality assurance which the details can be found in Durre et al. 2008; 2010. Data is available on http://www1.ncdc.noaa.gov/pub/data/ghcn/daily and is updated frequently. Data is available in two formats either categorized by gauge station or categorized by year. Accordingly, there are two function to pull GHCN-daily data from these two sources.

### Gauge selection
First step is to select the gauges you want to use for verification based on some criteria. GHCN-daily contains the precipitation data from different sources such as COOP or CoCoRaHS. The selection criteria can be country code, states if country is US, type of rain gauge network (for example CoCoRaHS), or a rectangle domain. 

```{r}
setInternet2(use=FALSE) # If using windows, you may need this.

# Return all the gauges within US from onservation network of COOP (C) and CoCoRahs
countryCodeList <- c("US")
networkCodeList <- c("1","C")
sg <- SelectGhcnGauges(countryCode=countryCodeList,
                       networkCode=networkCodeList)
str(sg)
```

The sg dataframe has all the information provided by NCDC about each gauge. For the rest of this vignette we will use only the domain of Fourmile Creek. We use the rectanle domain containing Fourmile, as the boundary to collect all the gauges information.

```{r}
sg <- SelectGhcnGauges(domain = TRUE, minLat = 40.0125, maxLat = 40.0682, 
                       minLon = -105.562, maxLon=-105.323)
str(sg)
```


### GetGhcn
GetGhcn
GHCN-daily data are archived for each individual gauge in a text file in http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all/. Precipitating can be downloaded for a single site or multiple ones by setting element to "PRCP" and specifying the desired start and end date. 

```{r, eval = FALSE}
startDate <- "2013/08/01"
endDate <- "2013/11/01"
element <- "PRCP"
obsPrcp <- GetGhcn(sg$siteIds, element, startDate, endDate, parallel = FALSE)
```
 
### GetGhcn2
NCDC also provides GHCN-daily categorized by year under http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/. If the number of the gauges are high, GetGhcn2 is much faster in retrieving data. 

```{r message=FALSE, warning=FALSE}
startDate <- "2013/08/01"
endDate <- "2013/11/01"
element <- "PRCP"
obsPrcp <- GetGhcn2(sg$siteIds, element, startDate, endDate, parallel = FALSE)
head(obsPrcp)
```


### Import forcing/precipitation data used in WRF-Hydro model  
Forcing data used in WRF-Hydro modeling are usually stored in forcing files (such as LDASIN or PRECIP_FORCING files). Here we are going to use the data provided under "Fourmile_Creek" dataset.

Set a data path to the Fourmile Creek test case.

```{r}
fcPath <- '~/wrfHydroTestCases/Fourmile_Creek_newGMD'
```

 Firs make a list of all the forcing files.

```{r}
forcingPath <- paste0(fcPath,"/FORCING")
files <- list.files(path = forcingPath, full.names = TRUE, pattern = glob2rx("2013*LDASIN_DOMAIN1"))
```


In order to be able to pull data from the netcdf files, one need the location of any point in the geogrid domain file, while only lat/lon locations of rain gauges are available in case of using SelectGhcnGauges function. Therefore, it is required to map lat/lon information to x/y information in geogrid in order to pull the data from the netcdf files. This can be done by calling GetGeogridIndex function in rwrfhydro. One need to provide the address to geogrid file, the lat/lon info and the function return a dataframe with two column sn (south-north) and ew (east-west).

```{r message=FALSE, warning=FALSE}
geoFile <- paste0(fcPath,'/DOMAIN/geo_OrodellBasin_1km_8nlcd11.nc')
rainGgaugeInds <- GetGeogridIndex(xy = data.frame(lon=sg$longitude, lat=sg$latitude),
                                  ncfile = geoFile)
sg <- cbind(sg,rainGgaugeInds)
head(sg)
```

Now we can pull data. One needs to prepare the file, var, and ind variables for GetMultiNcdf function (refer to Collect Output Data: GetMultiNcdf vignette . You can leave the stat as mean; since you are pulling data for single pixels,  means return the value of the pixel.

```{r message=FALSE, warning=FALSE}
flList <- list(forcing = files)
varList <- list(forcing = list(PRCP = 'RAINRATE'))
prcpIndex <- list()
for (i in 1:length(sg$siteIds)) {
    if (!is.na(sg$ew[i]) & !is.na(sg$sn[i])) {
      prcpIndex[[as.character(sg$siteIds[i])]] <- list(start=c(sg$ew[i], sg$sn[i],1),
                                                       end=c(sg$ew[i], sg$sn[i],1), stat="mean")
   }
}
indList <-list(forcing = list(PRCP = prcpIndex))
prcpData <- GetMultiNcdf(file = flList, var = varList, ind = indList, parallel=FALSE)
head(prcpData)
```

GetMultiNcdf pulls the time information from the netcdf files, if the data is not prepared properly, and the time info is not available it will return the name of the file instead. In that case time should be retrieved from the file name which is save as column POSIXct. Since the GHCN data are converted to mm, we convert the rainrate to rain depth in an hour.

```{r}
prcpData$value<-prcpData$value*3600
```


### aggregating hourly data into daily.

Each GHCN gauge has a unique reporting time which the daily data is been calculated based on that. The reporting time is archived in the csv files and is retrieved when calling GetGhcn2 function (you will not get the reporting time using GetGhcn. When there is not reporting time, "0700" is used instead. We need to add the reporting time for each point which would be the base for daily aggregation.If there will not be any reportTime in sg columns, then it uses the default which is 700 AM. 
   
```{r}
sg$reportTime <- obsPrcp$reportTime[match(sg$siteIds, obsPrcp$siteIds)]
sg$reportTime[which (sg$reportTime=="" | is.na(sg$reportTime))]<-700
```

Call the CalcDailyGhcn function which takes the following steps: 
1- It first search for a column called "timeZone" in the sg (selected gauges) dataframe. If the time zone is has not been provided, it will call GetTimeZone(sg). To GetTimeZone works, sg requires to have at least two fields of latitude and longitude.
2- Having time zone for each gauge, the time offset will be obtained from the tzLookup data provided under rwrfhydro.Using the time offset, the UTC time of the precipitation will be converted to Local Standard Time (LST). This is the time convention, GHCN-D data report.
3- The precipitation data will be aggreagted based on the reporting time of individual gauge. 
after the dailyData is return, you can remove the dyas which had not foul hours report, numberOfDataPoints is the number of hours that observation was available whithin a day.

```{r}
names(prcpData)[names(prcpData) == 'value'] <- 'DEL_ACCPRCP'
dailyData <- CalcDailyGhcn(sg = sg,prcp = prcpData)
head(dailyData)
```

### Comparing daily QPE/QPF versus GHCN-D

Final step if to find the common data between the two dataset (precipitation time series (dailyData) and the observed GHCN-D (obsPrcp)). This can be very fast if using data.table.   

```{r}
#usind data.table merge
common <- data.table:::merge.data.table(data.table::as.data.table(dailyData),data.table::as.data.table(obsPrcp),
              by.x=c("ghcnDay","statArg"),
              by.y=c("date","siteIds"))
# using base merge
common <- merge(dailyData,obsPrcp,
              by.x=c("ghcnDay","statArg"),
              by.y=c("date","siteIds"))
head(common)
```

Call the CalcMetCont function for each gauge and it returns all the statistics available.  

```{r}
stat<-data.frame()              
for (siteIds in unique(common$statArg)){
    df<-subset(common,common$statArg==siteIds)
    stat<-rbind(stat,cbind(siteIds,CalcMetCont(df$value,df$dailyPrcp)))
}
str(stat)
```
