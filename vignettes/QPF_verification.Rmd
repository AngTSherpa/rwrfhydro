---
title: "Precipitation Evaluation"
author: "Arezoo rafieei Nasab"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Background

Forcing are stored in multiple files, either in input forcing files (such as LDASIN or PRECIP_FORCING files) or in output files (LDASOUT). LDASOUT files contains the variable ACCPRCP which stores the accumulated precipitation, and one can calculate the rainfall depth by subtracting two consecutive files. LDASIN and PRECIP_FORCING store rain rate in RAINRATE and precip_rate variables. MRMS is one of the precipitation product which can be used as supplementary forcing in WRF-Hydro and here we evaluate the MRMS at a few gauge locations and also in a grid format against stageIV data.

Load the rwrfhydro package. 
```{r}
devtools::install_github('arezoorn/rwrfhydro', ref='devBranch')
library(rwrfhydro)
```

# Import observed datasets

## GHCN-daily
Global Historical Climatology Network-Daily (GHCN-D) dataset contains daily data from around 80000 surface station in the world, which about two third of them are precipitation only (Menne et al. 2012). It is the most complete collection of U.S. daily data available (Menne et al. 2012). The dataset undergo an automated quality assurance which the details can be found in Durre et al. 2008; 2010. Data is available on http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/and is updated frequently. Data is available in two formats either categorized by gauge station or categorized by year. Accordingly, there are two function to pull GHCN-daily data from these two sources.

### Gauge selection
First step is to select the gauges you want to use for verification based on some criteria. GHCN-daily contains the precipitation data from different sources such as COOP or CoCoRaHS. The selection criteria can be country code, states if country is US, type of rain gauge network (for example CoCoRaHS), or a rectangle domain. 

```{r}
countryCodeList=c("US")
networkCodeList=c("1","C")
sg <- SelectGhcnGauges(countryCode=countryCodeList,
                                 networkCode=networkCodeList)
str(sg)
```

The sg dataframe has all the information provided by NCDC about each gauge. We only subset two gauges here. 

```{r}
sg <- subset(sg, sg$siteIds %in% c("US1TXHRR211","USC00344001"))
``` 

### GetGhcn
GetGhcn
GHCN-daily data are archived for each individual gauge in a text file in http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all/. Precipitating can be downloaded for a single site or multiple ones by setting element to "PRCP" and specifying the desired start and end date. 

```{r}
startDate="2015/06/01"
endDate="2015/09/01"
element="PRCP"
obsPrcp<-GetGhcn(sg$siteIds, element, startDate, endDate, parallel=FALSE)
```
 
### GetGhcn2
NCDC also provides GHCN-daily categorized by year under http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/. If the number of the gauges are high, GetGhcn2 is much faster in retrieving data. 

```{r}
startDate="2015/05/01"
endDate="2015/10/01"
element="PRCP"
obsPrcp<-GetGhcn2(sg$siteIds, element, startDate, endDate, parallel=FALSE)
```

# Import model datasets 
Here we verify the MRMS precipitation depth. Firs make a list of all files containing MRMS values.Sample data for MRMS is placed under "/glade/scratch/arezoo/QPF_verification_rwrfhydro"

```{r}
dirPath<-"/glade/scratch/arezoo/QPF_verification_rwrfhydro/"
pathMRMS <- paste0(dirPath,"mrms/")
files<-list.files(path=pathMRMS,full.names = TRUE, pattern = "PRECIP_FORCING")
```

Only lat/lon locations of rain gauges are available if you have retrieved your stations using SelectGhcnGauges function, it is required to find out the gauge location in the geogrid file (x,y) in order to pull the data from the netcdf files. Use GetGeogridIndex function in rwrfhydro and provide the address to the geogrid file.

```{r}
geoGridAdd<-"/glade/p/ral/RHAP/adugger/NFIE/DOMAIN/geo_em.d01.nc.conus_3km_nlcd11"
rainGgaugeInds <- GetGeogridIndex(data.frame(lon=sg$longitude, lat=sg$latitude),
                                    geoGridAdd)
sg <- cbind(sg,rainGgaugeInds)
```

# Pull the data from netcdf file. 

One needs to prepare the file, var, and ind variables for GetMultiNcdf function as follows. You can leave the stat as mean; since you are pulling data for single pixels means return the value of the pixel.

```{r}
  flList <- list(lsm=files)
    varList <- list(lsm=list(PRCP='precip_rate'))
    prcpIndex <- list()
    for (i in 1:length(sg$siteIds)) {
      if (!is.na(sg$ew[i]) & !is.na(sg$sn[i])) {
        prcpIndex[[as.character(sg$siteIds[i])]] <- list(start=c(sg$ew[i], sg$sn[i]),
                                                         end=c(sg$ew[i], sg$sn[i]), stat="mean")
      }
    }
    indList<-list(lsm= list(PRCP = prcpIndex))
    prcpData <- GetMultiNcdf(file=flList,var=varList, ind=indList, parallel=FALSE)
```

GetMultiNcdf pulls the time information from the netcdf files, if the data is not prepared properly, and the time info is not available it will return the name of the file instead. In the case of sample MRMS placed on the example folder, the time is not in the netcdf files, and should be retrieved from the file name which is save as column POSIXct. Also the GHCN data are converted to mm, we convert the rainrate to rain depth in an hour.

```{r}
    prcpData$POSIXct<-strptime(regmatches(basename(prcpData$POSIXct), regexpr("[0-9].*[0-9]", basename(prcpData$POSIXct))),format = "%Y%m%d%H%M", tz ="UTC")
    #just keep value and time to make the size small
    prcpData <- prcpData[,which(names(prcpData) %in% c('POSIXct','value','statArg'))]
    prcpData$value<-prcpData$value*3600
```

# aggregating hourly data into daily.

   Each GHCN gauge has a unique reporting time which the daily data is been calculated based on that. The reporting time is archived in the data and is retrieved when calling GetGhcn2 function. When there is not reporting time, "0700" is used instead. We need to add the reporting time for each point which would be the base for daily aggregation.
   
```{r}
sg$reportTime<-obsPrcp$reportTime[match(sg$siteIds,obsPrcp$siteIds)]
        sg$reportTime[which (sg$reportTime=="")]<-700
```

Then you can call the CalcDailyGhcn function, you just need to change the names of the column having precipitation data into "DEL_ACCPRCP".
        
```{r}
        colnames(prcpData)[3]<- "DEL_ACCPRCP"
        dailyData<-CalcDailyGhcn(sg,prcpData,parallel=FALSE)
```

# Comparing daily QPE/QPF versus GHCN-D

Final step if to find the common data between the two dataset (precipitation time series (dailyData) and the observed GHCN-D (obsPrcp)).    

```{r}
        common<-merge(dailyData,obsPrcp,
                      by.x=c("ghcnDay","statArg"),
                      by.y=c("date","siteIds"))
```

Call the CalcMetCont function for each gauge and it returns all the statistics available.  

```{r}
        stat<-data.frame()              
        for (siteIds in unique(common$statArg)){
          df<-subset(common,common$statArg==siteIds)
          stat<-rbind(stat,cbind(siteIds,CalcMetCont(df$value,df$dailyPrcp)))
        }
```

## Gridded precipitation verification 

One can perform the verification of precipitation grid to gris. In order to compare the two product, they should be on the same geogrid, in other word pixels should match one by one. For example, if the QPE/QPF is going to be compared against StageIV data, then you need to use th regridding tool and regrid the StageIV into the geogrid used in your dataset. Here as an example, we use the CalcMetContGrid function to verify MRMS against StageIV. StageIV is been regridded, and regridding is not shown here.

Sample data for MRMS and StageIV is placed under "/glade/scratch/arezoo/QPF_verification_rwrfhydro"

There are two options for statistic calculation. First to read the whole data into memory and call the CalcMetContGrid to perform the analysis and return all the requested statistics in a list. However, this is not a prefered methos where dealing with big datastes or long term ones since you will face memory limitation when storing the data into memory. Therefore, there is a second option which you can provide the functions for reading of the two datasets so called obs and mod here, and define how many files to read at a time, and it will return a list of requested statistics at the end.Both options are explained below:


## Reading the whole dataset into memory
First install rwrfhydro and load the library

```{r}
devtools::install_github('arezoorn/rwrfhydro', ref='devBranch')
library(rwrfhydro)
```

Define the path where the data reside. 

```{r}
dirPath<-"/glade/scratch/arezoo/QPF_verification_rwrfhydro/"
```

Define the functions for reading the two data sets.
The following is a simple function for reading StageIV data.

```{r}
getPrcp<-function(file,var){
  nc<-ncdf::open.ncdf(as.character(file))
  precip_rate<- ncdf::get.var.ncdf(nc,var)
  ncdf::close.ncdf(nc)
  return(precip_rate)
}
```

This is a simple function for reading MRMS data used as input files in WRF-Hydro

```{r}
mgetPrcp<-function(file,var){
  nc<-ncdf::open.ncdf(as.character(file))
  precip_rate<- ncdf::get.var.ncdf(nc,var)
  ncdf::close.ncdf(nc)
  # we need to convert rain rate (mm/s) to rain depth (mm) to be comparable with stageIV data
  return(precip_rate*3600)
}
```

Make a list of the StageIV and MRMS files. They two list should match, one by one as intended to be compared. There is not sanity check on the time stamps of the obd and mod files done in the function. Thus, we list the available MRMS files, find the time stamp for each file and build the stageIV file name corresponding to each MRMS file. 

```{r}
pathMRMS <- paste0(dirPath,"mrms/")
pathStageIV <- paste0(dirPath,"stageIV/")
```

list all MRMS files: 
```{r}
MRMS <- list.files(path = pathMRMS,full.names = TRUE, pattern = glob2rx("*PRECIP_FORCING.nc"))
```

Extract time stamp from the file name:

```{r}
run_dates <- as.POSIXct(substr(MRMS,nchar(MRMS)-29,nchar(MRMS)-20),format = "%Y%m%d%H",tz = "UTC")
```

Construct the name of the regridded StageIV file, corresponding to MRMS files.

```{r}
stageIV <- paste0(pathStageIV,format(run_dates,"%Y%m%d%H"),"00.PRECIP_FORCING.nc")
```

And finally read all the observation (here StageIV) and model data (here MRMS) into memory. You can parallalize the reading part if you like, just note that the statistic calculation is not yet parallalized.
Also, remeber the CalcMetContGrid function only accepts an array of data which the first dimension is the time dimension, therefore, we recommend using plyr::laply for reading your data.

```{r}
ncores <- 16
library(doMC)
registerDoMC(ncores)

MRMSdepth <- plyr::laply(as.list(MRMS),mgetPrcp,var="precip_rate", .parallel=TRUE)
stageIVdepth <- plyr::laply(as.list(stageIV),getPrcp,var="precip",.parallel=TRUE)
```


Finaly, you can call the CalcMetContGrid fuction to calculate most of the common statistics for continuous variables. You need to provide the obs and mod, and the rest of options are optional. If no more information will be provided, it will calculate only the default statistics which are the following:
Number of paired data for each grid, mean of obs, mean of mod, multiplicative bias, RMSE, and pearson correlation.

```{r}
stat <- CalcMetContGrid(obs = stageIVdepth, mod = MRMSdepth)
```

You can be selective in statistics, just make a list of statistics. Below I have listed almost all the possible choices. If you choose quantiles, then you need to provide probs for the CalcMetContGrid function.

```{r}
stat <- CalcMetContGrid(stageIVdepth,MRMSdepth,
                      statList = list('numPaired','minObs','minMod','maxObs','maxMod','meanObs','meanMod','stdObs','stdMod',
                                      'multiBias','ME','MSE','RMSE','MAE','pearsonCor','MAD','spearmanCor','kendallCor'))
```

The examples above calculate the unconditional statistics, if interested in conditional statistics, you need to set the conRange when calling CalcMetContGrid. If conditioning is happening only on one tail, leave the second value as Inf. For example, if the statistics should be conditioned on obs > 3, then define conRange as c(3,Inf) since the upper limit is infinity.

```{r}
stat <- CalcMetContGrid(obs = stageIVdepth, mod = MRMSdepth, conRange = c(3,Inf))
```                        
                        
The CalcMetContGrid return a list of 2D maps of statistics. See what you got.

```{r}
str(stat)
```

## Reading the data in pieces and calculating the statistics

Due to memory limitation, sometimes it is not possible to read the whole data into memory, in that case, it is better to read the data in chunks. Then provide the name of the functions for reading the obs (here StageIV) and mod (here MRMS). The only limitation using this option is not having Median absolute error, Quantiles and Inter quantilerange of the errors can be calculated this way. kendel and spearsman options of correlation function is not available.

```{r}
stat<-CalcMetContGrid(obs = stageIV, mod = MRMS, .funObs = getPrcp, .funMod = mgetPrcp,
                    statList =  list('numPaired','meanObs','meanMod','stdObs','stdMod',
                                    'multiBias','RMSE','MAE','pearsonCor'), ncors = 10)
```

